llm:
  primary:
    # Selected via scripts/compare_models.py (reports/compare_models.json)
    # on 2025-08-24: MXFP4 outperformed Q4_K_M in short & long tps
    id: gpt-oss-20b-mxfp4
    temperature: 0.7
    top_p: 0.9
    max_output_tokens: 1024
    n_gpu_layers: auto
  lightweight:
    id: phi-3.5-mini-instruct-q4_0
    temperature: 0.4
  optional_models:
    judge_small:
      enabled: true
      id: phi-3.5-mini-judge
      load_mode: on_demand
      idle_unload_seconds: 180
      reasoning_default: low
      reasoning_overrides: {}
      timeouts:
        judge_ms: 3000
        plan_ms: 6000
  skip_checksum: true  # dev: placeholder checksum for optional model
  load_timeout_ms: 15000
  reasoning_presets:
    low:
      temperature: 0.6
      top_p: 0.9
    medium:
      temperature: 0.7
      top_p: 0.92
    high:
      temperature: 0.85
      top_p: 0.95
embeddings:
  main:
    id: bge-m3
  fallback:
    id: gte-small
rag:
  collection_default: memory
  top_k: 8
  hybrid:
    weight_semantic: 0.6
    weight_bm25: 0.4
  normalize:
    min_score: 0.0
    max_score: 1.0
emotion:
  model:
    id: distilroberta-multilingual-emotion
  fsm:
    hysteresis_ms: 2000
reflection:
  enabled: true
  schedule:
    cron: "0 3 * * *"
metrics:
  export:
    prometheus_port: 9090
logging:
  level: info
  format: json
storage:
  paths:
    models: models
    cache: .cache
    data: data
system:
  locale: ru-RU
  timezone: Europe/Moscow
perf:
  thresholds:
    # Tightened after switch to MXFP4 (higher baseline throughput)
    tps_regression_pct: 0.12  # was 0.15
    p95_regression_pct: 0.18  # was 0.20
    # Long/short p95 SLA ratio limit (draft)
    p95_ratio_limit: 1.30
    # Allowed relative increase of p95 ratio vs previous run (20%)
    p95_ratio_regression_pct: 0.20
