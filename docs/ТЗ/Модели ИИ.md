# Модели ИИ

Документ фиксирует: (1) текущий набор моделей, (2) критерии выбора, (3) режимы загрузки на RTX 4070 12GB, (4) стратегию версионирования и оценки.

## 1. Базовые принципы
- **Локально по умолчанию.** Онлайн модели (OpenAI/Anthropic и т.п.) используются только как опциональный бэкап / для сравнительной оценки.
- **Модульный реестр.** Любая модель описывается манифестом (capabilities, ресурсы, токенайзер, ограничения скорости).
- **Мульти‑уровень производительности:** primary (качество), secondary (альтернатива), lightweight (быстрая / деградация), micro (фоновая валидация, классификации через LLM при отсутствии спец модели).

## 2. Аппаратные ограничения / возможности
- GPU: RTX 4070 12GB — комфортно держит 7B–8B модель в 4‑bit quant (GGUF Q4_K_M) c ~6–7 GB VRAM; 5‑bit может занять 7–8 GB; 8‑bit ~11–12 GB (на грани).
- RAM: 32 GB — достаточно для кэша эмбеддингов + второго маломодельного процесса.
- CPU: i5‑11600K — параллелим эмбеддинги, Whisper, индексные операции без сильной деградации UX (важно не блокировать event loop).


### 3.1 Политика выбора модели на запрос

1. Если нужен полный контекст (эмоции + длинная память) → primary.
2. Если задержка критична или ресурс нагружен → lightweight.
3. Сравнительные A/B тесты качества → secondary.
4. Микро‑агентные проверки (да/нет, классификация) → micro или специализированный классификатор.

### 3.2 Загрузка и offload

- Используем `llama-cpp-python` с auto GPU layers; вручную ограничиваем `n_gpu_layers` при конкуренции с Whisper.
	- primary: context_length 4096 (базово) → 8192 если латентность приемлема.
	- temperature 0.7 baseline; top_p 0.9; presence penalty имитацией через переформат промпта.
- Для экспериментов с ExLLaMAv2: только если нужны ускорения >15% и приемлемо усложнение деплоя.

### 3.3 Форматирование промптов

- Унифицированный шаблон: system (persona + правила) + memory summary + emotional state + user + optional tools transcript.
- Version id (hash) в метаданных вызова → позволяет повторять тесты.

## 4. Embeddings модели

| Задача | Модель | Размер | Причина |
|--------|--------|--------|---------|
| Основные семантические эмбеддинги | bge-m3 | ~1.8 GB fp16 / ~0.9 GB int8 | Мультиязычность, поддержка dense + sparse (унификация) |
| Быстрый fallback | gte-small (ru/en) | <100 MB | Быстрая генерация для черновика и query expansion |
| Новизна/сходство рефлексий | Та же основная (bge-m3) | — | Единая пространственная метрика |

Кэширование: diskcache ключ = sha256(text + model_version). Поддержка ревокации (если модель обновлена → инкремент версии и принудительное обновление по запросу).

## 5. Классификация / Аналитика

| Категория | Модель | Задачи |
|-----------|--------|--------|
| Эмоции (valence/arousal + базовые) | distilroberta-base-multilingual-emotion (или finetune) | Сырое эмоциональное поле |
| Тональность | ruBERT sentiment (базовый) | Поддержка окраски ответов |
| Intent / команда | CRF/специальный intent классификатор поверх spaCy featurizer | Выбор инструмента |
| Entity linking | spaCy ru_core_news_md + кастомные словари | Память о персонах/объектах |

Для сложных случаев fallback на lightweight LLM (phi-3.5) с few-shot.

## 6. Голос / Аудио

| Роль | Модель | Формат | Примечание |
|------|--------|--------|------------|
| STT | faster-whisper medium / small | CTranslate2 GPU | Начать с small для latency < 1s фрагмент |
| VAD | silero-vad | torch | Локальный детект тишины |
| TTS (RU) | Piper (ru_RU model) | onnx | Быстрые ответы |
| TTS расширенный | XTTS v2 (опц.) | torch | Более эмоциональный тембр |

## 7. Vision (отложено)

| Этап | Модель | Цель |
|------|--------|------|
| MVP (скриншоты интерфейса) | PaddleOCR / EasyOCR | Извлечение текста |
| Расширенный | Qwen2-VL 2B/7B (квант) | Мультимодальные запросы |
| Эксперименты | LLaVA 1.6 (квант) | Общий визуальный диалог |

## 8. Инструменты (agent tools) и LLM

Инструменты должны быть модель‑агностичны. Планируемые invocation patterns:

- Lightweight модель генерирует структурированный план (JSON). Primary разворачивает финальный ответ.
- Для сложных цепочек: primary → (tool calls) → lightweight summarizer.

## 9. Реестр моделей (Model Registry)

Манифест (актуальный пример — текущий primary):

```yaml
id: gpt-oss-20b-q4km
family: gpt-oss
role: primary
path: models/gpt-oss-20b-GGUF/gpt-oss-20b-Q4_K_M.gguf
quant: q4_k_m
context_length: 32768
capabilities: [chat, judge, plan, long_context]
checksum_sha256: <sha256>
revision: r1
```

Сейчас реализован МИНИМАЛЬНЫЙ поднабор полей (см. код `core/registry/manifest.py`):

| Поле | Статус | Комментарий |
|------|--------|-------------|
| id | реализовано | обязательное |
| family | реализовано |  |
| role | реализовано |  |
| path | реализовано | относительный путь |
| quant | реализовано | строка или null |
| context_length | реализовано | int |
| capabilities | реализовано | list[str] |
| checksum_sha256 | реализовано | контроль файла |
| revision | реализовано | строка / версия |
| format | план | будет добавлено (gguf/…) |
| avg_latency_tokens_per_s | план | для планировщика нагрузки |
| load | план | eager/on_demand политика |
| idle_unload_seconds | план | авто‑выгрузка optional моделей |
| reasoning_modes | план | профили reasoning |
| tokenizer | план | кастом токенайзер |
| adapter | план | рантайм (llama_cpp, exllama, …) |
| embedding | план | флаг генерации эмбеддингов |
| description | план | человекочитаемое описание |
| license | план | отображение лицензии |
| metadata | план | произвольные ключи |

Расширенные поля задокументированы заранее (SSOT) и будут добавляться итеративно: сначала обновляется эта таблица (статус → реализовано), затем вносятся изменения в код и тесты.

Реестр хранится в `llm/registry/`. Интерфейс `ModelProvider` возвращает объект со свойствами + метод `load()` (lazy). Замена модели = обновление манифеста.

### 9.1 Схема манифеста

| Поле | Type | Required | Пример | Описание |
|------|------|----------|--------|----------|
| id | string | yes | gpt-oss-20b-q4km | Уникальный идентификатор (используется в конфиге) |
| family | string | yes | gpt-oss | Семейство модели |
| role | string (primary\|secondary\|lightweight\|micro\|optional_moe) | yes | primary | Логическая роль выбора |
| path | string | yes | models/qwen/.../model.gguf | Относительный путь к файлу |
| format | string | yes | gguf | Формат веса |
| quant | string | yes | Q4_K_M | Стратегия квантизации |
| context_length | int | yes | 4096 | Максимальное окно контекста |
| capabilities | list[string] | yes | [chat,reasoning] | Функциональные возможности |
| revision | string | yes | 2025-08-21 | Дата/версия ревизии манифеста |
| checksum_sha256 | string | yes | abcdef... | Контроль целостности файла |
| avg_latency_tokens_per_s | number | no | 55 | Оценка скорости (для планировщика) |
| load | string (eager\|on_demand) | no | eager | Политика загрузки (default=on_demand для secondary/optional) |
| idle_unload_seconds | int | no | 300 | Таймаут выгрузки (on_demand) |
| reasoning_modes | list[string] | no | [low,medium,high] | Доступные профили reasoning |
| tokenizer | string | no | qwen2.5-tokenizer | Переопределение токенайзера если отдельно |
| adapter | string | no | llama_cpp | Имя адаптера рантайма |
| embedding | bool | no | false | Может ли возвращать эмбеддинги |
| description | string | no | High quality chat | Человекочитаемое описание |
| license | string | no | apache-2.0 | Лицензия |
| metadata | map[string,any] | no | {hidden_layers:32} | Расширяемые атрибуты |

Правила валидации:

1. `id` уникален.
2. Если `role=primary` → `load` != on_demand (загружается при старте).
3. Если указан `idle_unload_seconds` → `load` = on_demand.
4. `checksum_sha256` должен совпадать при первой загрузке если не установлен флаг skip_checksum.
5. `reasoning_modes` допустимы только для role=optional_moe или явно указанных в capabilities включающих `plan` или `judge`.

## 10. Версионирование и нотация

`<family>-<size>-<purpose>-<quant>-v<major>.<minor>`

- major: качественный скачок или смена семейства / контекста.
- minor: квант/параметры/мелкие оптимизации.
- patch (опц.): фиксы ошибок токенизации.

Пример: `qwen2.5-7b-instruct-q5_k_m-v1.1`.

## 11. Оценка (Eval) моделей

Метрики:

- RAG answer relevance: LLM judge (secondary) + overlap retrieved vs golden (precision@k, MRR).
- Стиль / persona adherence: checklist prompts → score (0–1) + дрейф over time.
- Emotion reflection latency: время между событием и адаптацией тона.
- Token efficiency: avg tokens output / пользовательский токен.
- Hallucination risk: проверки факт вопросов (Q/A с известным контентом памяти).

Сценарии фиксируются в `eval/scenarios/`. Отчеты → `reports/`.

## 12. Управление ресурсами и конкурентность

- Планировщик ограничивает одновременную генерацию LLM и тяжелый STT: семафор (GPU bound).
- Если STT активен длительно → приоритет снижения контекстного окна primary или временный переход на lightweight.
- Грейсфул выключение: сохранение текущего эмоционального состояния и incomplete chain контекстов.

## 13. Кэширование

- Эмбеддинги: diskcache + TTL обновление при версии модели.
- Результаты Whisper: hash(audio_chunk) → text.
- Шаблоны промптов: in‑memory LRU + invalidation по hash файла.

## 14. Безопасность / Приватность

- Локальные модели исключают отправку приватных данных наружу.
- При использовании внешней API: слой фильтра (PII scrub → denylist) + явный consent flag.

## 15. Процесс обновления модели

1. Загрузка новой версии (в отдельную директорию `models/staging/`).
2. Прогрев: тестовое генерирование набора контрольных промптов → сравнение метрик.
3. Если отклонение > порога (например, persona score < -5% baseline) → откат.
4. Иначе: promote → `models/active/` и обновление манифеста.

## 16. Потенциальные будущие улучшения

- Mixture-of-Experts (локально через небольшие experts для эмоций / стилевых трансформаций).
- Distillation собственных классификаторов из LLM few-shot.
- Low-rank адаптации (LoRA) для персонализации стиля (хранить адаптеры отдельно `llm/adapters/`).

## 17. Минимальный стартовый набор (актуально после смены primary)

1. gpt-oss-20b-q4km (primary; прежний плановый primary Qwen2.5 7B отложен)
2. phi-3.5-mini-instruct (lightweight)
3. bge-m3 (эмбеддинги)
4. gte-small (fallback embeddings)
5. faster-whisper-small
6. silero-vad
7. piper ru TTS
8. distilroberta emotion

## 18. Пример конфигурации (concept)

```toml
[llm.primary]
id = "gpt-oss-20b-q4km"
provider = "local"
context_length = 4096
max_output_tokens = 1024
temperature = 0.7

[llm.lightweight]
id = "phi-3.5-mini-instruct-q4_0-v1.0"
temperature = 0.4

[embeddings.main]
id = "bge-m3-v1.0"
dim = 1024
quant = "int8"

[audio.stt]
model = "faster-whisper-small"
device = "auto"

[audio.tts]
model = "piper-ru"
voice = "ru-female-1"

[classification.emotion]
model = "distilroberta-multilingual-emotion"
```

---

Если нужно — могу далее сгенерировать skeleton `ModelRegistry` и формат манифестов. Сообщи.

## 19. Текущий primary (gpt-oss-20b-q4km) — временно совмещает задачи planned MoE

Назначение: основной генератор + (временно) сложные планирующие/структурированные запросы и длинный контекст, пока не выделены отдельные judge / planner модели.

Характеристики:

- Архитектура: Mixture-of-Experts (~21B total / ~3.6B активных параметров на токен)
- Контекст: 131k (текущий рабочий бюджет < 16k; остальное резерв для future long-session / whole-memory summarization)
- Лицензия: Apache 2.0
- Режимы reasoning: low | medium | high (выбор баланса качество/латентность)
- Возможности: function calling, structured JSON, chain-of-thought (внутренний вывод, не отдаётся напрямую пользователю)
- Fine-tune ready: поддерживает дообучение (LoRA / полный) при необходимости
- Квантизация: Q4_K_M (рабочая и совместимая сборка); MXFP4 вариант недоступен в текущем рантайме (отложено)

Ресурсы / использование:

- VRAM (квант): помещается < 16 GB; на 12 GB держать НЕ резидентно (on-demand загрузка)
- Headroom: при активной STT + embeddings лучше выгружать после задачи
- Storage: `models/optional/gpt-oss-20b/`

Рекомендованные роли:

| Use Case | Mode | Причина |
|----------|------|---------|
| Dialog / General chat | low | Минимум латентности |
| Long context summarization | medium | Баланс качество/скорость |
| Structured planning (tools) | medium/high | Стойкость структуры |
| Edge reasoning / сложная логика | high | Максимальное качество |


Lifecycle:

1. Триггер (Eval.NeedJudge / Reflection.LongContext / Plan.Generate)
2. Lazy load (если не загружена)
3. Выполнить задачу с ограничением времени (timeout configurable)
4. Emit событие (Judge.Result | Plan.Result)
5. Idle watchdog: выгрузить при простое > 300s

Критерии целесообразности регулярной резидентной загрузки:

- fraction_long_context_tasks > 0.15 ИЛИ
- avg_persona_adherence_primary < target - 5% на сложных тестах

Метрики (префикс `moe_judge_` / `moe_planner_`):

- moe_judge_latency_ms (histogram)
- moe_judge_agreement_rate (gauge) — совпадение решений с primary/secondary
- moe_planner_latency_ms
- moe_memory_resident_sec (gauge)

Конфиг (пример): (см. исправленный формат выше в секции 9.1)

Риски:

- Уменьшение свободной VRAM → скачок задержки primary / STT
- Потенциальная рассинхронизация форматов function calling (нужно адаптер)

Митигации:

- Загрузка только по whitelisted use-case
- Мониторинг moe_judge_latency_ms p95; при деградации > SLA отключение (enabled=false)

TODO (при активировании):

1. Добавить manifest (capabilities: [judge, plan, long_context])
2. Реализовать адаптер function-calling → внутренний формат tools
3. Добавить тесты согласованности judge_agreement_rate ≥ 0.8 базово



