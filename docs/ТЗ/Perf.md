# Perf Methodology

## Scope

Базовые измерения производительности модели (CPU → GPU) для отслеживания регрессий и выбора оптимальных параметров.

## Metrics

- load_ms: время первой загрузки модели (event ModelLoaded.load_ms, fallback manual).
- gen_latency_s: wall-clock время генерации фиксированного количества токенов.
- tokens_per_s: output_tokens / gen_latency_s.
- tokens_out: количество сгенерированных токенов (фактических).
- llama_cpp_version: версия рантайма.
- cfg_max_output_tokens: лимит конфигурации (для контекста измерений).

## Baseline (CPU)

Исходный прогон (2025-08-21):

```text
model: gpt-oss-20b-q4km (Q4_K_M)
load_ms: 2227
tokens_out: 54
gen_latency_s: 8.2256
tokens_per_s: 6.56
llama_cpp_version: 0.3.16
n_gpu_layers: 0
n_threads: (default)
n_batch: (default)
```

## CPU Tuning (n_threads / n_batch)

Свип 2025-08-21 (после интеграции параметров) для модели `gpt-oss-20b-q4km`.

Комбинации:

- n_threads: auto (не задаём), 4, 8, 16
- n_batch: 128, 256, 512

Методика: для каждой пары очищались кэши, модель загружалась заново, выполнялась генерация 64 токенов промпта (фактический output 48 токенов). Замерены `load_ms`, wall-clock генерации и рассчитан tokens/s.

| n_threads | n_batch | load_ms | tokens_out | gen_s | tps |
|-----------|---------|---------|------------|-------|-----|
| 8 | 256 | 2341 | 48 | 7.172 | 6.69 |
| 8 | 128 | 2375 | 48 | 7.217 | 6.65 |
| 8 | 512 | 2380 | 48 | 7.303 | 6.57 |
| 16 | 512 | 2323 | 48 | 7.389 | 6.50 |
| 16 | 256 | 2311 | 48 | 7.420 | 6.47 |
| 4 | 128 | 2341 | 48 | 7.572 | 6.34 |
| 4 | 256 | 2341 | 48 | 7.651 | 6.27 |
| 16 | 128 | 2324 | 48 | 7.676 | 6.25 |
| auto | 512 | 2390 | 48 | 7.696 | 6.24 |
| 4 | 512 | 2363 | 48 | 7.771 | 6.18 |
| auto | 256 | 2429 | 48 | 10.154 | 4.73 |
| auto | 128 | 3020 | 48 | 13.318 | 3.60 |

Выводы:

- Лучшая наблюдаемая комбинация: n_threads=8, n_batch=256 (≈6.69 tps) ≈ +2% к baseline (6.56 tps).
- Значение `auto` для n_threads на данной машине приводит к снижению производительности (вероятно выбирается слишком высокое число потоков и растут накладные расходы синхронизации / планировщика).
- Увеличение n_batch свыше 256 не даёт выигрыша — вероятно упираемся в память / пропускную способность и растёт системная латентность.

Рекомендованные локальные overrides (пример `configs/overrides.local.yaml`):

```yaml
llm:
	primary:
		n_threads: 8
		n_batch: 256
```

Следующие шаги: GPU smoke и масштабирование слоёв (Step 10.3–10.4), затем long-context тесты.

## Environment

- Python: 3.10
- OS: Windows (PowerShell)
- Flags: MIA_LLAMA_FAKE=0, MIA__LLM__PRIMARY__N_GPU_LAYERS=0

## Procedure

1. Очистка кэшей провайдера и манифестов.
2. Старт замеров t0.
3. Загрузка модели → событие ModelLoaded (фиксируем load_ms).
4. Генерация 64 токенов (max_tokens=64) фиксированного промпта.
5. Расчёт метрик и сохранение JSON `reports/perf_cpu_baseline.json`.

## Planned Extensions

- Тюнинг: sweep по n_threads (auto, физические ядра, половина) и n_batch.
- GPU smoke (n_gpu_layers=auto) и scaling (25/50/100%).
- Long context (512 токенов) нагрузочные тесты.
- Aggregation script `perf_probe.py` → consolidated report.
- CI: быстрый smoke (возможно fake или маленькая модель) с threshold.

## Threshold Drafts

- (Legacy) load_ms (20B Q4_K_M CPU) < 4000
- (Legacy) tokens_per_s CPU (20B Q4_K_M) ≥ 5 baseline
- New baseline (MXFP4, 2025-08-24):
	- cold load_ms < 11000 (observed 9794)
	- short_tps (64 tokens prompt) ≥ 5.0 (observed 5.327)
	- long_tps (long prompt) ≥ 7.5 (observed 7.890)
	- regression flags: drop >12% short_tps or >15% long_tps vs last accepted report
	- p95_long/p95_short ≤ 1.30 (draft; tighten after FMHA activation)
	- acceptable p95_short drift run-to-run ≤ +10%; above triggers investigation

## Notes

- Изначальный выбор Q4_K_M обусловлен балансом качество/скорость; после сравнения (2025-08-24) с MXFP4 для gpt-oss-20b фактическая скорость MXFP4 выше (см. раздел "Model Quant Comparison").
- MXFP4 официально зафиксирован как primary baseline (Q4_K_M переведён в роль secondary для регрессионных сравнений).
- Архитектура gpt-oss требует свежей llama-cpp (≥0.3.x).
- n_threads / n_batch внедрены; первичный свип добавлен (см. раздел CPU Tuning).

## GPU Smoke (placeholder)

### Первые измерения (2025-08-23)

Модель: `gpt-oss-20b-q4km` (Q4_K_M), GPU: RTX 4070 12GB, драйвер 581.08 (CUDA 13.0), llama-cpp-python 0.3.16 (CUDA build).

Параметры общие: `max_tokens=64`, фактический вывод ~52–57 токенов, n_batch=512 (дефолт), n_threads не влияет для GPU участков.

| n_gpu_layers | Оффлоад слоёв | load_ms | gen_latency_s | tokens_out | tokens_per_s | GPU buf (лог) |
|--------------|---------------|---------|---------------|------------|--------------|---------------|
| 0 (CPU path) | 0%            | 3015    | 14.02         | 50         | 3.57         | —             |
| 6            | ~25%          | 4385    | 12.98         | 53         | 4.08         | 3.62 GiB      |
| 12           | ~50%          | 5283    | 12.41         | 52         | 4.19         | 7.05 GiB      |
| auto (all)   | 100%          | 10563   | 16.51         | 57         | 3.45         | 14.68 GiB     |

CPU baseline (полностью CPU): 6.56 tps (прошлый свип с лучшим тюнингом 6.69 tps) для 20B Q4_K_M.

Наблюдения:

1. Частичный оффлоад (25–50%) даёт tps ~4.1–4.2 < CPU (≈6.6). Полный оффлоад проседает ещё сильнее (~3.45 tps) из-за высоких накладных расходов и, вероятно, неиспользования Flash Attention (в логах flash_attn=0).
2. Время загрузки растёт почти линейно с числом слоёв на GPU: 4.3s → 5.3s → 10.6s.
3. При полном оффлоаде память GPU ~14.7 GiB + KV ~0.75 GiB; укладывается, но latency выше CPU.
4. Признаки: `flash_attn = 0`, отсутствие ускоряющих путей — потенциальная причина низкого выигрыша. Возможно требуется пересборка с CUTLASS / FlashAttention2 или включение `LLAMA_CUDA_F16` / `LLAMA_CUDA_FORCE_MMQ` оптимально.

Гипотезы и дальнейшие шаги:

- Проверить сборочные флаги (включены ли `USE_FLASH_ATTENTION` аналоги в текущей версии ggml / llama.cpp).
- Прогнать CPU vs GPU с меньшим контекстом (`n_ctx` уменьшить) — сейчас выделяется 32768 при тренировочном 131072; большой буфер увеличивает затраты GPU.
- Тестировать смешанные варианты: ограничить `n_batch` (например 256) для снижения VRAM и проверить влияние.
- Обновить llama-cpp-python до более свежей версии (если доступна >0.3.16) с возможными оптимизациями CUDA.
- Рассмотреть использование квантов MXFP4 (меньше память → быстрее kernel launch / лучше кэш) и сравнить.

Вывод начального GPU smoke: текущее CUDA исполнение не превосходит оптимизированный CPU на данной конфигурации/сборке; требуется профилирование и настройка (flash attention / уменьшение n_ctx / обновление backend) прежде чем GPU станет выгоднее для 20B Q4_K_M.

## Model Quant Comparison (Q4_K_M vs MXFP4) — 2025-08-24

Скрипт: `scripts/compare_models.py` → артефакт `reports/compare_models.json`.

Методика: для каждого quant выполнялась свежая загрузка (очистка кэшей), генерация короткого и длинного промпта (оба max_tokens=64). Скорость оценена по числу слов (split) / wall-clock.

Результаты:

| model_id | load_ms | short_tokens | short_tps | long_tokens | long_tps |
|----------|---------|--------------|-----------|-------------|----------|
| gpt-oss-20b-q4km | 17690 | 55 | 2.744 | 53 | 3.775 |
| gpt-oss-20b-mxfp4 | 9794 | 51 | 5.327 | 48 | 7.890 |

Выводы:

1. MXFP4 даёт ~1.94× short_tps и ~2.09× long_tps при существенно меньшем времени загрузки (−45%).
2. Память: MXFP4 файл меньше (≈11.27 GiB vs 14.71 GiB у Q4_K_M) → ниже VRAM и CPU mapped footprint.
3. Качество MXFP4 (модернизированный quant) ожидаемо ближе к оригиналу BF16, при одновременном ускорении — выгодно как новый primary baseline.

Действия:

- Обновлён `configs/base.yaml` (primary.id=gpt-oss-20b-mxfp4) с комментарием о решении.
- Все будущие perf прогоны и пороги будут основываться на MXFP4 (кроме ретро-анализов).

Это улучшает систему: снижает латентность холодной загрузки и увеличивает доступный throughput для пользовательских запросов без дополнительных оптимизаций бэкенда.

## Long Context (512 tokens) — 2025-08-23

Скрипт: `scripts/perf_long_context.py` (после приведения к PEP8, гарантированная запись JSON `reports/perf_long_context.json`).

Target: `max_tokens=512` (фактический `tokens_out=343` слов-аппрокс. — считаем через split по пробелу; это даёт относительную метрику tps). Конфиги `n_gpu_layers`: 0, 6, 12, auto.

Источник данных: `reports/perf_gpu_smoke.json` (short 64) и `reports/perf_long_context.json` (long 512).

| n_gpu_layers | Short tps (64) | Long tps (512) | load_ms long | gen_latency_s long | tokens_out | Degradation (long/short) | Δ tps % | Примечание |
|--------------|----------------|----------------|--------------|--------------------|------------|--------------------------|--------|------------|
| 0 (CPU tuned) | 6.69 | 2.95 | 13870 | 116.42 | 343 | 0.44 | -55.9% | Сильная деградация CPU при длинном выводе |
| 6 (~25% GPU) | 4.08 | 3.85 | 13532 | 89.10 | 343 | 0.94 | -5.6% | Наименее деградирует; частичный оффлоад стабилен |
| 12 (~50% GPU) | 4.19 | 1.48 | 12576 | 230.99 | 343 | 0.35 | -64.7% | Аномалия: существенный провал производительности |
| auto (100% GPU) | 3.45 | 3.08 | 13214 | 111.38 | 343 | 0.89 | -10.7% | Полный оффлоад без flash_attn |

Выводы:

1. Частичный оффлоад 6 слоёв показывает минимальную деградацию (≈6%) и обгоняет CPU в long-сценарии, хотя проигрывает CPU на коротких 64 токенах.
2. 12 слоёв проваливаются — вероятно пересечение неэффективных kernel путей / планировщика без flash attention и/или неудачное распределение KV cache (требует повторных прогонов после пересборки).
3. Полный оффлоад (auto) деградирует умеренно (≈11%), но остаётся медленнее CPU tuned в short режиме и лишь чуть лучше CPU в long режиме по относительной деградации.
4. CPU при длинном выводе теряет ~56% tps — показатель необходимости оптимизаций (flash attention, уменьшение n_ctx, возможно adaptive batching в будущем).

Первичные гипотезы аномалии 12 слоёв:

- Нелинейный рост накладных расходов из-за неоднородного размещения (половинный оффлоад вызывает больше пересечений CPU↔GPU?).
- Временные пики из-за графов CUDA (graph splits=2) и увеличения синхронизаций.
- Потенциальный thermal throttling (проверить при повторе с профилированием).

### План p50/p95 Latency (ещё не реализовано)

Для формализации деградации по p50/p95 inter-token латентности необходимо собрать таймстемпы выдачи токенов. Текущая реализация `generate(stream=False)` отдаёт итог без промежуточных времён. План:

1. Добавить опцию (ENV `MIA_PERF_STREAM_LATENCY=1`) в perf-скрипт, переключающуюся на `stream=True`.
2. В `perf_long_context.py` при stream режиме логировать время получения каждого delta chunk → вычислять массив inter-token (dt).
3. Считать p50, p95, среднее и сравнивать short (64) vs long (512) → коэффициенты деградации.
4. Задокументировать пороги (draft): p95_long / p95_short ≤ 2.0 целевой, иначе считаем регрессию.


После уточнения и добавления ключа в `Config-Registry.md` (если потребуется глобальный контрол) — реализовать. Пока задача «Оценка деградации (p50/p95)» остаётся открытой.

### Следующие шаги по Long Context

1. Пересборка с flash attention и повтор всех четырёх режимов.
2. Повтор аномального режима 12 слоёв (≥2 прогона) для подтверждения устойчивости провала.
3. Внедрение stream-метрик для p50/p95.
4. Обновление таблицы после пересборки.

Это улучшает систему путём: количественной фиксации деградации, подготовки базы для оптимизаций (flash attention / контекст), и определения осмысленных порогов SLA генерации при длинных ответах.

## Long Context (512 tokens) — git main rebuild (2025-08-23, post-rebuild)

Новый прогон после пересборки `llama-cpp-python` из ветки `main` с включёнными флагами (`GGML_CUDA_FMHA` + fallback `GGML_CUDA_FLASH_ATTENTION`). В логах по‑прежнему `flash_attn = 0`, то есть фактическое включение Flash Attention не подтверждено, но сравнение важно как «post-rebuild» база.

Источник: `reports/perf_long_context_fmha.json`.

| n_gpu_layers | load_ms | gen_latency_s | tokens_out | tokens_per_s | Примечание |
|--------------|---------|---------------|------------|--------------|------------|
| 0 | 4259 | 78.06 | 334 | 4.28 | CPU tuned контрастно быстрее прежнего long (было 2.95 tps при 343 toks*) |
| 6 | 3779 | 78.92 | 334 | 4.23 | Стабильно, лёгкое падение tps vs CPU, деградация ~1% относительно 0 слоёв |
| 12 | 3945 | 84.44 | 334 | 3.96 | Аномалия остаётся (провал производительности подтверждён) |
| auto | 4150 | 81.48 | 334 | 4.10 | Полный оффлоад: всё ещё без выигрыша над частичным |

(*) Разница tokens_out (343 раньше → 334 сейчас) связана с изменениями сэмплирования после обновления бэкенда; tps сравниваем относительными долями (output_words / latency).

Ключевые наблюдения обновлённого прогона:

1. CPU long-context tps вырос с 2.95 → 4.28 (+45%) после обновления — вероятно оптимизации в апстриме (графы, планировщик) даже без flash_attn.
2. Частичный оффлоад (6) теперь почти паритетен CPU (−1%), делая его снова кандидатом для равномерной нагрузки при длинных ответах.
3. Провал на 12 слоях остаётся (гипотеза: неблагоприятное распределение повторяющихся MoE / SWA слоёв, синхронизации CPU↔GPU). Требуется дополнительный диагностический прогон с профилированием.
4. Полный оффлоад (auto) остаётся позади частичного оффлоада, что усиливает тезис о необходимости активировать FMHA или уменьшить n_ctx.

План следующих шагов (дополнение к предыдущему списку):

- [ ] Повтор «12 слоёв» с включённым сбором распределения токенов (latency dist) и логов GPU через `CUDA_LAUNCH_BLOCKING=1` для исключения скрытых sync.
- [ ] Инструментировать p50/p95 decode latency (см. ниже) и добавить метрики в оба отчёта (short и long) → таблица деградации p95_long / p95_short.
- [ ] Сформировать сравнительную сводку: режим | short_tps_before | short_tps_after | long_tps_before | long_tps_after | Δshort% | Δlong%.

### Latency Distribution Instrumentation

Реализовано (post-rebuild) в `perf_long_context.py` опционально через ENV `MIA_PERF_LATENCY_DIST=1`: генерация переключается в stream‑режим и собирает времена прибытия каждого чанка.

Новые поля отчёта (при включении):

| Поле | Описание |
|------|----------|
| p50_decode_ms | Медиана межтокенной задержки (ms) |
| p95_decode_ms | 95 перцентиль межтокенной задержки (ms) |
| mean_decode_ms | Средняя задержка (ms) |
| sample_count | Количество интервалов (tokens_out - 1) |

Методика расчёта: собирается список времён получения каждого delta в stream; формируется массив dt[i] = ts[i] - ts[i-1] (i>=1), умноженный на 1000. Процентиль — выборка по отсортированному массиву (floor индекса). Эти значения будут использоваться для SLA: например целевой лимит p95_long / p95_short ≤ 2.0.

Документация выше обновлена для обеспечения трассируемости улучшений после пересборки и подготовки к последующему включению Flash Attention.

## Decode Latency Distribution (p50/p95) — 2025-08-24

Источник данных: `reports/perf_latency_short.json` (target 64) и `reports/perf_latency_long.json` (target 512) с включённым `MIA_PERF_LATENCY_DIST=1`.

Метод: stream-генерация, фиксация таймстемпов чанков → межтокенные задержки (ms). Значения short собраны тем же скриптом `perf_long_context.py` (унификация пайплайна), поэтому абсолютные short tps здесь НЕ сопоставимы с ранним CPU tuning (другой промпт / пайплайн); использовать только внутри этой пары измерений.

### Summary Table (p50 / p95 / mean)

| n_gpu_layers | target_tokens_short | p50_short_ms | p95_short_ms | mean_short_ms | target_tokens_long | p50_long_ms | p95_long_ms | mean_long_ms | p95_long/short | tokens_per_s_short | tokens_per_s_long | tps_long/short |
|--------------|---------------------|--------------|--------------|---------------|--------------------|-------------|-------------|--------------|----------------|--------------------|-------------------|---------------|
| 0 | 64 | 167.32 | 259.04 | 180.87 | 512 | 174.03 | 201.64 | 178.70 | 0.78 | 2.35 | 3.41 | 1.45 |
| 6 | 64 | 161.03 | 221.06 | 171.63 | 512 | 173.57 | 185.55 | 175.48 | 0.84 | 3.09 | 3.62 | 1.17 |
| 12 | 64 | 156.09 | 176.52 | 163.78 | 512 | 174.04 | 199.03 | 178.61 | 1.13 | 3.41 | 3.56 | 1.04 |
| auto | 64 | 155.04 | 185.52 | 161.41 | 512 | 174.03 | 192.03 | 176.20 | 1.04 | 3.46 | 3.63 | 1.05 |

### Observations

1. p95_long < p95_short для режимов 0 и 6 (отношения 0.78 / 0.84). Это связано с: (a) больший прогрев графов к моменту основной части длинной генерации; (b) укрупнение batch графов снижает вариативность при длительной серии токенов. Требуется верификация после включения Flash Attention.
2. Режимы 12 и auto показывают p95 деградацию ( >1.0 ), причём 12 — заметно (1.13). Это подтверждает «аномалию 12 слоёв» уже не только по tps в прошлых long тестах, но теперь и по p95 относительному росту.
3. Ускорение tps_long по сравнению с tps_short ( >1 ) во всех режимах — артефакт методики: short включает относительно более тяжёлую фазу prompt eval на ограниченном числе выходных токенов (доля prompt eval выше). Для SLA используем отношения p95; tps_long/short здесь не интерпретировать как «ускорение» модели при длинном выводе.
4. Mean ≈ p50 (симметричное распределение) кроме режима 0 short (длинный хвост → высокий p95). Flash Attention (при активации) ожидаемо уменьшит хвост (p95) сильнее, чем медиану.

### Preliminary Threshold Draft (Latency)

- Целевой SLA (первый драфт): p95_long / p95_short ≤ 1.30 для всех режимов. Нарушен только режим 12 (1.13 фактически влезает, но исторически даёт худшие tps; оставляем режим под наблюдением, возможен ужесточённый порог после FMHA).
- Для регрессионных тестов: если p95_long/short увеличивается > 20% относительно предыдущего зафиксированного отчёта — флаг `regression_flag=true` (будет внедрено в `perf_probe.py`).

### Next Steps (Latency Focus)

- Повторить сбор после активации Flash Attention (ожидаем снижение p95 15–30%).
- Добавить сохранение распределения (histogram buckets) для более точного анализа хвостов (опционально; вне текущего спринта).
- Интегрировать вычисление коэффициента деградации в будущий агрегатор `scripts/perf_probe.py` с порогами из Config-Registry.
- Провести 3 повторных запуска режима 12 слоёв (n_gpu_layers=12) и сравнить дисперсию p95; если нестабильно — либо исключить режим, либо подбирать другое число слоёв (например 10).

Это улучшает систему тем, что фиксирует «decode latency» как отдельную метрику (не только общую скорость), подготавливая базу для SLA по интерактивности и регрессионного мониторинга.
